<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Classical k-gram Language Models in R • kgrams</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Classical k-gram Language Models in R">
<meta property="og:description" content="kgrams">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">kgrams</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/kgrams.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="kgrams_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Classical k-gram Language Models in R</h1>
            
      
      
      <div class="hidden name"><code>kgrams.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kgrams</span><span class="op">)</span></code></pre></div>
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
</div>
<div id="building-a-k-gram-language-model" class="section level2">
<h2 class="hasAnchor">
<a href="#building-a-k-gram-language-model" class="anchor"></a>Building a k-gram language model</h2>
<p>This section illustrates the typical workflow for building a k-gram language model with <code>kgrams</code>. In summary, this involves the following main steps:</p>
<ol style="list-style-type: decimal">
<li>Load the training corpus, i.e. the text from which k-gram frequencies are estimated.</li>
<li>Preprocess the corpus and tokenize sentences.</li>
<li>Store k-gram frequency counts from the preprocessed training corpus.</li>
<li>Build the final language model, by initializing its parameters and compute eventual auxiliary counts required by the smoothing technique employed.</li>
</ol>
<p>We illustrate all these steps in the following.</p>
<div id="step-1-loading-the-training-corpus" class="section level3">
<h3 class="hasAnchor">
<a href="#step-1-loading-the-training-corpus" class="anchor"></a>Step 1: Loading the training corpus</h3>
<p><code>kgrams</code> offers two options for reading the text corpora used in its computations, which are basically in-memory and out-of-memory solutions:</p>
<ul>
<li>
<em>in-memory</em>. The corpus is simply loaded in the R session as a <code>character</code> vector.</li>
<li>
<em>out-of-memory</em>. The text is read in batches of fixed size from a <code>connection</code>. This solution includes, for instance, reading text from a file, from an URL, or from the standard input.</li>
</ul>
<p>The out-of-memory solution can be useful for training over large corpora without the need of storing the entire text into the RAM.</p>
<p>In this vignette, for illustration, we will train a language model on an online text source. We will use William Shakespeare’s “Much Ado About Nothing” (text of the entire play) as training corpus, which is freely available on <a href="http://shakespeare.mit.edu">shakespeare.mit.edu</a>:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Create an URL connection to Shakespeare's "Much Ado About Nothing" </span>
<span class="va">txt_con</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/connections.html">url</a></span><span class="op">(</span><span class="st">"http://shakespeare.mit.edu/much_ado/full.html"</span><span class="op">)</span></code></pre></div>
</div>
<div id="step-2-preprocessing-and-tokenizing-sentences" class="section level3">
<h3 class="hasAnchor">
<a href="#step-2-preprocessing-and-tokenizing-sentences" class="anchor"></a>Step 2: preprocessing and tokenizing sentences</h3>
<p>One can (and usually should) apply some transformations to the raw training corpus before feeding it as input to the k-gram counting algorithm. In particular, the algorithm considers as a single sentence each entry of its pre-processed input, and pads each sentence with Begin-Of-Sentence (BOS) and End-Of-Sentence (EOS) tokens.</p>
<p>In this step, we only need to <em>define</em> the functions used for preprocessing and sentence tokenization. We will use the following functions, which leverage on the basic utilities <code><a href="../reference/preprocess.html">kgrams::preprocess()</a></code> and <code>kgrams::.tknz_sent()</code>, and perform some additional step, since we will be reading raw HTML lines from the URL connection created above.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">.preprocess</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># Remove speaker name and locations (boldfaced in original html)</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"&lt;b&gt;[A-z]+&lt;/b&gt;"</span>, <span class="st">""</span>, <span class="va">x</span><span class="op">)</span>
        <span class="co"># Remove other html tags</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"&lt;[^&gt;]+&gt;||&lt;[^&gt;]+$||^[^&gt;]+&gt;$"</span>, <span class="st">""</span>, <span class="va">x</span><span class="op">)</span>
        <span class="co"># Apply standard preprocessing including lower-case</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">kgrams</span><span class="fu">::</span><span class="fu"><a href="../reference/preprocess.html">preprocess</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
        <span class="co"># Collapse to a single string to avoid splitting into more sentences at the end of lines</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">x</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span>
        <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">.tknz_sent</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># Tokenize sentences</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">kgrams</span><span class="fu">::</span><span class="fu"><a href="../reference/tknz_sent.html">tknz_sent</a></span><span class="op">(</span><span class="va">x</span>, keep_first <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
        <span class="co"># Remove empty sentences</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">x</span> <span class="op">!=</span> <span class="st">""</span><span class="op">]</span>
        <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
</div>
<div id="step-3-get-k-gram-frequency-counts" class="section level3">
<h3 class="hasAnchor">
<a href="#step-3-get-k-gram-frequency-counts" class="anchor"></a>Step 3: get k-gram frequency counts</h3>
<p>We can now obtain k-gram frequency counts from Shakespeare with a single command, using the function <code><a href="../reference/kgram_freqs.html">kgram_freqs()</a></code>. The following stores k-gram counts for k-grams of order less than or equal to five:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">freqs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kgram_freqs.html">kgram_freqs</a></span><span class="op">(</span><span class="va">txt_con</span>, <span class="co"># Read Shakespeare's text from connection</span>
                     N <span class="op">=</span> <span class="fl">5</span>, <span class="co"># Store k-gram counts for k &lt;= 5</span>
                     .preprocess <span class="op">=</span> <span class="va">.preprocess</span>,  <span class="co"># preprocess text</span>
                     .tknz_sent <span class="op">=</span> <span class="va">.tknz_sent</span>, <span class="co"># tokenize sentences</span>
                     verbose <span class="op">=</span> <span class="cn">FALSE</span>, <span class="co"># If TRUE, prints current progress info </span>
                     max_lines <span class="op">=</span> <span class="cn">Inf</span>, <span class="co"># Read until the end-of-file</span>
                     batch_size <span class="op">=</span> <span class="fl">100</span> <span class="co"># Read text in batches of 100 lines</span>
                     <span class="op">)</span>
<span class="va">freqs</span>
<span class="co">#&gt; A k-gram frequency table.</span></code></pre></div>
<p>The object <code>freqs</code> is an object of class <code>kgram_freqs</code>, i.e. a k-gram frequency table. We can obtain a first informative summary of what this object contains by calling <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">freqs</span><span class="op">)</span>
<span class="co">#&gt; A k-gram frequency table.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt; * N: 5</span>
<span class="co">#&gt; * V: 3046</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of words in training corpus:</span>
<span class="co">#&gt; * W: 26174</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of distinct k-grams with positive counts:</span>
<span class="co">#&gt; * 1-grams:3048</span>
<span class="co">#&gt; * 2-grams:14389</span>
<span class="co">#&gt; * 3-grams:21208</span>
<span class="co">#&gt; * 4-grams:22706</span>
<span class="co">#&gt; * 5-grams:22975</span></code></pre></div>
<p>The parameter <code>V</code> is the size of the dictionary, which was created behind the scenes by <code><a href="../reference/kgram_freqs.html">kgram_freqs()</a></code>, using all words encountered in the text. In alternative, we could have used a pre-specified dictionary through the argument <code>dict</code>, and specify whether new words (not present in the original dictionary) should be added to it, or be replaced by an Unknown-Word (UNK) token, by the argument <code>open_dict</code>; see <code><a href="../reference/kgram_freqs.html">?kgram_freqs</a></code> for further details. The number of distinct unigrams is greater than the size of the dictionary, because the former also includes the special BOS and EOS tokens.</p>
<p>Notice that the functions <code>.preprocess()</code> and <code>.tknz_sent()</code> we defined above are passed as arguments of <code>kgram_freqs</code><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. These are also saved in the final data structure, and are by default applied also to input at prediction time.</p>
<p>The following shows how to query k-gram counts from the frequency table created above <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Query some simple unigrams and bigrams</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"leonato"</span>, <span class="st">"enter leonato"</span>, <span class="st">"thy"</span>, <span class="st">"smartphones"</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 38  6 52  0</span>
<span class="co"># Query k-grams at the beginning or end of a sentence</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="../reference/EOS.html">BOS</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%+%</span> <span class="fu"><a href="../reference/EOS.html">BOS</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%+%</span> <span class="st">"i"</span>, <span class="st">"love"</span> <span class="op">%+%</span> <span class="fu"><a href="../reference/EOS.html">EOS</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 208   0</span>
<span class="co"># Total number of words processed</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="st">""</span><span class="op">)</span> 
<span class="co">#&gt; [1] 26174</span>
<span class="co"># Total number of sentences processed</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="../reference/EOS.html">EOS</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> 
<span class="co">#&gt; [1] 2270</span></code></pre></div>
<p>The most important use of <code>kgram_freqs</code> objects is to create language models, as we illustrate in the next step.</p>
</div>
<div id="step-4--build-the-final-language-model" class="section level3">
<h3 class="hasAnchor">
<a href="#step-4--build-the-final-language-model" class="anchor"></a>Step 4. Build the final language model</h3>
<p><code>kgrams</code> provides support for creating language models using several classical smoothing techniques. The list of smoothers supported by <code>kgrams</code> can be retrieved through:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/smoothers.html">smoothers</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] "ml"    "add_k" "abs"   "kn"    "mkn"   "sbo"   "wb"</span></code></pre></div>
<p>The documentation page <code><a href="../reference/smoothers.html">?smoothers</a></code> provides a list of original references for the various smoothers. We will use <a href="https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing">Interpolated Kneser-Ney</a> smoothing, which goes under the code <code>"kn"</code>. We can get some usage help for this method through the command:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/smoothers.html">info</a></span><span class="op">(</span><span class="st">"kn"</span><span class="op">)</span>
<span class="co">#&gt; Interpolated Kneser-Ney</span>
<span class="co">#&gt;  * code: 'kn'</span>
<span class="co">#&gt;  * parameters: D</span>
<span class="co">#&gt;  * constraints: 0 &lt;= D &lt;= 1</span></code></pre></div>
<p>As shown above, Kneser-Ney has one parameter <code>D</code>, which is the discount applied to bare k-gram frequency counts or continuation counts. We will use <code>D = 0.75</code>.</p>
<p>To train a language model with the k-gram counts stored in <code>freqs</code>, use:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/language_model.html">language_model</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="st">"kn"</span>, D <span class="op">=</span> <span class="fl">0.75</span><span class="op">)</span>
<span class="va">kn</span>
<span class="co">#&gt; A k-gram language model.</span></code></pre></div>
<p>This will create a <code>language_model</code> object, which can be used to obtain word continuation and sentence probabilities. Let us first get a summary of our final model:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; A k-gram language model.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Smoother:</span>
<span class="co">#&gt; * 'kn'.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt; * N: 5</span>
<span class="co">#&gt; * V: 3046</span>
<span class="co">#&gt; * D: 0.75</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of words in training corpus:</span>
<span class="co">#&gt; * W: 26174</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of distinct k-grams with positive counts:</span>
<span class="co">#&gt; * 1-grams:3048</span>
<span class="co">#&gt; * 2-grams:14389</span>
<span class="co">#&gt; * 3-grams:21208</span>
<span class="co">#&gt; * 4-grams:22706</span>
<span class="co">#&gt; * 5-grams:22975</span></code></pre></div>
<p>The parameter <code>D</code> can be accessed and modified through the functions <code><a href="../reference/parameters.html">parameters()</a></code> and <code><a href="../reference/parameters.html">param()</a></code>, which have a similar interface to the base R function <code><a href="https://rdrr.io/r/base/attributes.html">attributes()</a></code> and <code><a href="https://rdrr.io/r/base/attr.html">attr()</a></code>:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/parameters.html">parameters</a></span><span class="op">(</span><span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; $N</span>
<span class="co">#&gt; [1] 5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $V</span>
<span class="co">#&gt; [1] 3046</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $D</span>
<span class="co">#&gt; [1] 0.75</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.75</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.6</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">0.75</span></code></pre></div>
<p>We can also modify the order of the k-gram model, by choosing any number less than or equal to <code>N = 5</code> (since we stored up to 5-gram counts):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="co"># 'kn' uses only 1:4-grams</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span>
<span class="co">#&gt; [1] 4</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="co"># 'kn' uses also 5-grams</span></code></pre></div>
<p>In the next section we show how to use this language model for basic tasks such as predicting word and sentence probabilities, and for more complex tasks such as computing perplexities and generating random text.</p>
</div>
</div>
<div id="using-language_model-objects" class="section level2">
<h2 class="hasAnchor">
<a href="#using-language_model-objects" class="anchor"></a>Using <code>language_model</code> objects</h2>
<p>So far we have created a <code>language_model</code> object using Interpolated Kneser-Ney as smoothing method. In this section we show how to:</p>
<ul>
<li>Obtain word continuation and sentence probabilities.</li>
<li>Generate random text by sampling from the language model probability distribution.</li>
<li>Compute the language model’s perplexity on a test corpus.</li>
</ul>
<div id="word-continuation-and-sentence-probabilities" class="section level4">
<h4 class="hasAnchor">
<a href="#word-continuation-and-sentence-probabilities" class="anchor"></a>Word continuation and sentence probabilities</h4>
<p>We can obtain both sentence probabilities and word continuation probabilities through the function <code><a href="../reference/probability.html">probability()</a></code>. This is generic on the first argument, which can be a <code>character</code> for sentence probabilities, or a <code>word_context</code> expression for continuation probabilities.</p>
<p>Sentence probabilities can be obtained as follows (the first two are sentences from the training corpus):</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/probability.html">probability</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Did he break out into tears?"</span>,
              <span class="st">"I see, lady, the gentleman is not in your books."</span>,
              <span class="st">"We are predicting sentence probabilities."</span>
              <span class="op">)</span>,
            model <span class="op">=</span> <span class="va">kn</span>
            <span class="op">)</span>
<span class="co">#&gt; [1] 2.720954e-05 8.628460e-07 9.230391e-19</span></code></pre></div>
<p>Behind the scenes, the same <code>.preprocess()</code> and <code>.tknz_sent()</code> functions used during training are being applied to the input. We can turn off this behaviour by explicitly specifying the <code>.preprocess</code> and <code>.tknz_sent</code> arguments of <code><a href="../reference/probability.html">probability()</a></code>.</p>
<p>Word continuation probabilities are the conditional probabilities of words following some given context. For instance, the probability that the words <code>"tears"</code> or <code>"pieces"</code> will follow the context <code>"Did he break out into"</code> are computed as follows:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/probability.html">probability</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"tears"</span>, <span class="st">"pieces"</span><span class="op">)</span> <span class="op">%|%</span> <span class="st">"Did he break out into"</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; [1] 9.992621e-06</span></code></pre></div>
<p>The operator <code><a href="../reference/word_context.html">%|%</a></code> takes as input a character vector on its left-hand side, i.e. the candidate words, and a length one character vector on its right-hand side, i.e. the given context. If the context has more than <code>N - 1</code> words (where <code>N</code> is the order of the language model, five in our case), only the last <code>N - 1</code> words are used for prediction.</p>
</div>
<div id="generating-random-text" class="section level4">
<h4 class="hasAnchor">
<a href="#generating-random-text" class="anchor"></a>Generating random text</h4>
<p>We can sample sentences from the probability distribution defined by our language model using <code><a href="../reference/sample_sentences.html">sample_sentences()</a></code>. For instance:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">840</span><span class="op">)</span>
<span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "i have studied officers ; &lt;EOS&gt;"                                              </span>
<span class="co">#&gt;  [2] "truly by in your company thing that you ask for [...] (truncated output)"     </span>
<span class="co">#&gt;  [3] "i protest i love the gentleman is wise ; &lt;EOS&gt;"                               </span>
<span class="co">#&gt;  [4] "for it . &lt;EOS&gt;"                                                               </span>
<span class="co">#&gt;  [5] "the best befits can i for your own hobbyhorses hence [...] (truncated output)"</span>
<span class="co">#&gt;  [6] "but by this travail fit the length july cham's beard [...] (truncated output)"</span>
<span class="co">#&gt;  [7] "don pedro she doth well as being some attires and [...] (truncated output)"   </span>
<span class="co">#&gt;  [8] "exeunt all ladies only spots of grey all the wealth [...] (truncated output)" </span>
<span class="co">#&gt;  [9] "heighho ! &lt;EOS&gt;"                                                              </span>
<span class="co">#&gt; [10] "exit margaret ursula friar . &lt;EOS&gt;"</span></code></pre></div>
<p>The sampling is performed word by word, and the output is truncated if no <code>EOS</code> token is found after sampling <code>max_length</code> words.</p>
<p>We can also sample with a temperature different from one. The temperature transformation of a probability distribution <span class="math inline">\(p(i)\)</span> is defined by:</p>
<p><span class="math display">\[p_t(i) = \dfrac{\exp(\log{p(i)} / t)} {Z(t)},\]</span> where <span class="math inline">\(Z(t)\)</span> is the partition function, defined in such a way that <span class="math inline">\(\sum _i p_t(i) \equiv 1\)</span>. Intuitively, higher and lower temperatures make the original probability distribution smoother and rougher, respectively. By making a physical analogy, we can think of less probable words as states with higher energies, and the effect of temperature is to make more likely to excite these high energy states.</p>
<p>We can test the effects of temperature on our Shakespeare-inspired language model:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>, 
                 t <span class="op">=</span> <span class="fl">0.1</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "i will not have to do with you . &lt;EOS&gt;"                                    </span>
<span class="co">#&gt;  [2] "i will go before and show him their examination . [...] (truncated output)"</span>
<span class="co">#&gt;  [3] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [4] "i will not have to do with you . &lt;EOS&gt;"                                    </span>
<span class="co">#&gt;  [5] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [6] "i will not think it . &lt;EOS&gt;"                                               </span>
<span class="co">#&gt;  [7] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [8] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [9] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt; [10] "i will not have to do with you . &lt;EOS&gt;"</span>
<span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>, 
                 t <span class="op">=</span> <span class="fl">10</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "pleasantspirited minds wants valuing peace speech libertines after offered being [...] (truncated output)"     </span>
<span class="co">#&gt;  [2] "braggarts smell fiveandthirty from possible knowest sickness tonight panders agony [...] (truncated output)"   </span>
<span class="co">#&gt;  [3] "show'd where's give intelligence princes finer tire scab brought rearward [...] (truncated output)"            </span>
<span class="co">#&gt;  [4] "deserved heart's evening virtues holds c hadst persuasion can finer [...] (truncated output)"                  </span>
<span class="co">#&gt;  [5] "churchbench modesty thinks noncome remorse epitaphs consented mortifying whom hath [...] (truncated output)"   </span>
<span class="co">#&gt;  [6] "expectation impossible yielded deceive wedding mouth unclasp absentand qualify twelve [...] (truncated output)"</span>
<span class="co">#&gt;  [7] "giddily certainly nightraven prized grief laugh claw invincible tyrant blessed [...] (truncated output)"       </span>
<span class="co">#&gt;  [8] "'i senseless beat time denies 'hundred ten forth hire' reenter [...] (truncated output)"                       </span>
<span class="co">#&gt;  [9] "toothache laughed civil kill'd mean hero's yea foundation deformed appetite [...] (truncated output)"          </span>
<span class="co">#&gt; [10] "hour studied figure nine leonato enough ever herself confess authority [...] (truncated output)"</span></code></pre></div>
<p>As explained above, sampling with low temperature gives much more weight to highly likely sentences, and indeed the output is very repetitive. On the contrary, high temperature makes sentence probabilities more uniform, and in fact our output above looks quite random.</p>
</div>
<div id="compute-language-models-perplexities" class="section level4">
<h4 class="hasAnchor">
<a href="#compute-language-models-perplexities" class="anchor"></a>Compute language model’s perplexities</h4>
<p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is a standard evaluation metric for the overall performance of a language model. It is given by <span class="math inline">\(P = e^H\)</span>, where <span class="math inline">\(H\)</span> is the cross-entropy of the language model sentence probability distribution against a test corpus empirical distribution:</p>
<p><span class="math display">\[
H = - \dfrac{1}{W}\sum _s\ \ln (\text {Prob}(s)) 
\]</span> Here <span class="math inline">\(W\)</span> is total number of words in the test corpus (counting EOS but not BOS tokens), and the sum extends over all sentences in the test corpus. Perplexity does not give direct information on the performance of a language model in a specific end-to-end task, but is often found to correlate with the latter, which provides a practical justification for the use of this metric. Notice that better models are associated with lower perplexities, and that <span class="math inline">\(H\)</span> is proportional to the negative log-likelihood of the corpus under the language model assumption.</p>
<p>Perplexities can be computed in <code>kgrams</code> using the function <code><a href="../reference/perplexity.html">perplexity()</a></code>, which can read text both from a <code>character</code> vector and from a <code>connection</code>. We will take our test corpus again from Shakespeare’s opus, specifically the play “A Midsummer Night’s Dream”, which is example data from <code>kgrams</code> namespace:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">midsummer</span><span class="op">[</span><span class="fl">840</span><span class="op">]</span>
<span class="co">#&gt; [1] " when truth kills truth o devilishholy fray !"</span></code></pre></div>
<p>We can compute the perplexity of our Kneser-Ney 5-gram model <code>kn</code> against this corpus as follows:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/perplexity.html">perplexity</a></span><span class="op">(</span><span class="va">midsummer</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; [1] 376.8554</span></code></pre></div>
<p>We can use perplexity to tune our model parameter <code>D</code>. We compute perplexity over a grid of values for <code>D</code> and plot the results. We do this for the k-gram models with <code>k = 2:5</code>:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">D_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.5</span>, to <span class="op">=</span> <span class="fl">1.0</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span>
<span class="va">FUN</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">D</span>, <span class="va">N</span><span class="op">)</span> <span class="op">{</span>
        <span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">N</span>
        <span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">D</span>
        <span class="fu"><a href="../reference/perplexity.html">perplexity</a></span><span class="op">(</span><span class="va">midsummer</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="op">}</span>
<span class="va">P_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">2</span><span class="op">:</span><span class="fl">5</span>, <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">FUN</span>, N <span class="op">=</span> <span class="va">N</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"n"</span>, xlab <span class="op">=</span> <span class="st">"D"</span>, ylab <span class="op">=</span> <span class="st">"Perplexity"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">300</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"chartreuse"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"blue"</span>,<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">4</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"black"</span>,<span class="op">)</span></code></pre></div>
<div class="figure">
<img src="kgrams_files/figure-html/unnamed-chunk-19-1.png" alt="Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models." width="50%"><p class="caption">
Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models.
</p>
</div>
<p>We see that the optimal choices for <code>D</code> are close to its maximum allowed value <code>D = 1</code>, for which the performance of the 2-gram model is slightly worse than the higher order models, and that the 5-gram model performs generally worse than the 3-gram and 4-gram models. Indeed, the optimized perplexities for the various k-gram orders are given by:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"2-gram"</span> <span class="op">=</span> <span class="fl">1</span>, <span class="st">"3-gram"</span> <span class="op">=</span> <span class="fl">2</span>, <span class="st">"4-gram"</span> <span class="op">=</span> <span class="fl">3</span>, <span class="st">"5-gram"</span> <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>,
       <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">P_grid</span><span class="op">[[</span><span class="va">N</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>
       <span class="op">)</span>
<span class="co">#&gt;   2-gram   3-gram   4-gram   5-gram </span>
<span class="co">#&gt; 324.8595 320.5757 319.9021 319.9982</span></code></pre></div>
<p>which shows that the best performing model is the 4-gram one, while it seems that the 5-gram model is starting to overfit (which is not very surprising, given the ridiculously small size of our training corpus!).</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2 class="hasAnchor">
<a href="#conclusions" class="anchor"></a>Conclusions</h2>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Strictly speaking, a single argument <code>.preprocess</code> would suffice, as the processed input is (symbolically) <code>.tknz_sent(.preprocess(input))</code>. Having two separate arguments for preprocessing and sentence tokenization has a couple of advantages, as explained in <code><a href="../reference/kgram_freqs.html">?kgram_freqs</a></code>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The string concatenation operator <code><a href="../reference/%+%.html">%+%</a></code> is equivalent to <code><a href="https://rdrr.io/r/base/paste.html">paste(lhs, rhs)</a></code>. Also, the helpers <code><a href="../reference/EOS.html">BOS()</a></code>, <code><a href="../reference/EOS.html">EOS()</a></code> and <code><a href="../reference/EOS.html">UNK()</a></code> return the BOS, EOS and UNK tokens, respectively.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Valerio Gherardi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
