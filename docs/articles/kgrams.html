<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Classical -gram Language Models in R • kgrams</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Classical -gram Language Models in R">
<meta property="og:description" content="kgrams">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">kgrams</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/kgrams.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="kgrams_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Classical <span class="math inline">\(k\)</span>-gram Language Models in R</h1>
                        <h4 class="author">Valerio Gherardi</h4>
            
      
      
      <div class="hidden name"><code>kgrams.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kgrams</span><span class="op">)</span></code></pre></div>
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p><code>kgrams</code> provides R users with a set of tools for training, tuning and exploring <span class="math inline">\(k\)</span>-gram language models<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. It gives support for a number of common Natural Language Processing (NLP) tasks: from the basic ones, such as extracting (<em>tokenizing</em>) <span class="math inline">\(k\)</span>-grams from a text and predicting sentence or continuation probabilities, to more advanced ones such as computing language model perplexities<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and sampling sentences according the language model’s probability distribution. Furthermore, it supports many classical <span class="math inline">\(k\)</span>-gram smoothing methods, including the well-known modified Kneser-Ney algorithm, first described in <span class="citation">(Chen and Goodman 1999)</span>, and widely considered the best performing smoothing technique for <span class="math inline">\(k\)</span>-gram models.</p>
<p><span class="math inline">\(k\)</span>-gram language models are notoriously demanding from the space point of view, and many of the toolkits available for <span class="math inline">\(k\)</span>-gram based NLP employ various techniques and data structures to achieve the data compression required by the large scales of industry (and, sometimes, academic) applications (see <span class="citation">(Pibiri and Venturini 2019)</span> for a recent review). On the other hand, at such large scales, <em>neural</em> language models are often the most economic and best performing choice, and this is likely to become more and more so in the future. In developing <code>kgrams</code>, I made no special attempt at data compression, and <span class="math inline">\(k\)</span>-grams and count estimates are stored in plain C++ STL hash-tables, which can grow rapidly large as the size of corpora and dictionaries increases.</p>
<p>On the other hand, most focus is put on providing a <em>fast</em>, time efficient implementation, with intuitive interfaces for text processing and for model evaluation, and a reasonably large choice of pre-implemented smoothing algorithms, making <code>kgrams</code> suitable for small- and medium-scale language model experiments, for rapidly building baseline models, and for pedagogical purposes.</p>
<p>In the following Sections, I illustrate the prototypical workflow for <a href="#kgram_model">building a <span class="math inline">\(k\)</span>-gram language model</a> with <code>kgrams</code>, show how to compute <a href="#probability">probabilities</a> and <a href="#perplexity">perplexities</a>, and (for the sake of fun!) <a href="#sampling">generate random text</a> at different temperatures.</p>
</div>
<div id="kgram_model" class="section level2">
<h2 class="hasAnchor">
<a href="#kgram_model" class="anchor"></a>Building a <span class="math inline">\(k\)</span>-gram language model</h2>
<p>This section illustrates the typical workflow for building a <span class="math inline">\(k\)</span>-gram language model with <code>kgrams</code>. In summary, this involves the following main steps:</p>
<ol style="list-style-type: decimal">
<li>Load the training corpus, i.e. the text from which <span class="math inline">\(k\)</span>-gram frequencies are estimated.</li>
<li>Preprocess the corpus and tokenize sentences.</li>
<li>Store <span class="math inline">\(k\)</span>-gram frequency counts from the preprocessed training corpus.</li>
<li>Build the final language model, by initializing its parameters and computing auxiliary counts possibly required by the smoothing technique employed.</li>
</ol>
<p>We illustrate all these steps in the following.</p>
<div id="step-1-loading-the-training-corpus" class="section level3">
<h3 class="hasAnchor">
<a href="#step-1-loading-the-training-corpus" class="anchor"></a>Step 1: Loading the training corpus</h3>
<p><code>kgrams</code> offers two options for reading the text corpora used in its computations, which are basically in-memory and out-of-memory solutions:</p>
<ul>
<li>
<em>in-memory</em>. The corpus is simply loaded in the R session as a <code>character</code> vector.</li>
<li>
<em>out-of-memory</em>. The text is read in batches of fixed size from a <code>connection</code>. This solution includes, for instance, reading text from a file, from an URL, or from the standard input.</li>
</ul>
<p>The out-of-memory solution can be useful for training over large corpora without the need of storing the entire text into the RAM.</p>
<p>In this vignette, for illustration, we will train a language model on an online text source. We will use William Shakespeare’s “Much Ado About Nothing” (text of the entire play) as training corpus, which is freely available on <a href="http://shakespeare.mit.edu">shakespeare.mit.edu</a>:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Create an URL connection to Shakespeare's "Much Ado About Nothing" </span>
<span class="va">txt_con</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/connections.html">url</a></span><span class="op">(</span><span class="st">"http://shakespeare.mit.edu/much_ado/full.html"</span><span class="op">)</span></code></pre></div>
</div>
<div id="step-2-preprocessing-and-tokenizing-sentences" class="section level3">
<h3 class="hasAnchor">
<a href="#step-2-preprocessing-and-tokenizing-sentences" class="anchor"></a>Step 2: preprocessing and tokenizing sentences</h3>
<p>One can (and usually should) apply some transformations to the raw training corpus before feeding it as input to the <span class="math inline">\(k\)</span>-gram counting algorithm. In particular, the algorithm considers as a <em>sentence</em> each entry of its pre-processed input, and pads each sentence with Begin-Of-Sentence (BOS) and End-Of-Sentence (EOS) tokens. It considers as a <em>word</em> any substring of a sentence delimited by (one or more) space characters.</p>
<p>For the moment, we only need to define the functions used for preprocessing and sentence tokenization. We will use the following functions, which leverage on the basic utilities <code><a href="../reference/preprocess.html">kgrams::preprocess()</a></code> and <code><a href="../reference/tknz_sent.html">kgrams::tknz_sent()</a></code>, and perform some additional steps, since we will be reading raw HTML lines from the URL connection created above.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">.preprocess</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># Remove speaker name and locations (boldfaced in original html)</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"&lt;b&gt;[A-z]+&lt;/b&gt;"</span>, <span class="st">""</span>, <span class="va">x</span><span class="op">)</span>
        <span class="co"># Remove other html tags</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">gsub</a></span><span class="op">(</span><span class="st">"&lt;[^&gt;]+&gt;||&lt;[^&gt;]+$||^[^&gt;]+&gt;$"</span>, <span class="st">""</span>, <span class="va">x</span><span class="op">)</span>
        <span class="co"># Apply standard preprocessing including lower-case</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">kgrams</span><span class="fu">::</span><span class="fu"><a href="../reference/preprocess.html">preprocess</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
        <span class="co"># Collapse to a single string to avoid splitting into more sentences at the end of lines</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">x</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span>
        <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">.tknz_sent</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># Tokenize sentences</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">kgrams</span><span class="fu">::</span><span class="fu"><a href="../reference/tknz_sent.html">tknz_sent</a></span><span class="op">(</span><span class="va">x</span>, keep_first <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
        <span class="co"># Remove empty sentences</span>
        <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">x</span> <span class="op">!=</span> <span class="st">""</span><span class="op">]</span>
        <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
</div>
<div id="step-3-get-k-gram-frequency-counts" class="section level3">
<h3 class="hasAnchor">
<a href="#step-3-get-k-gram-frequency-counts" class="anchor"></a>Step 3: get <span class="math inline">\(k\)</span>-gram frequency counts</h3>
<p>We can now obtain <span class="math inline">\(k\)</span>-gram frequency counts from Shakespeare with a single command, using the function <code><a href="../reference/kgram_freqs.html">kgram_freqs()</a></code>. The following stores <span class="math inline">\(k\)</span>-gram counts for <span class="math inline">\(k\)</span>-grams of order less than or equal to <span class="math inline">\(N = 5\)</span>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">freqs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kgram_freqs.html">kgram_freqs</a></span><span class="op">(</span><span class="va">txt_con</span>, <span class="co"># Read Shakespeare's text from connection</span>
                     N <span class="op">=</span> <span class="fl">5</span>, <span class="co"># Store k-gram counts for k &lt;= 5</span>
                     .preprocess <span class="op">=</span> <span class="va">.preprocess</span>,  <span class="co"># preprocess text</span>
                     .tknz_sent <span class="op">=</span> <span class="va">.tknz_sent</span>, <span class="co"># tokenize sentences</span>
                     verbose <span class="op">=</span> <span class="cn">FALSE</span>, <span class="co"># If TRUE, prints current progress info </span>
                     max_lines <span class="op">=</span> <span class="cn">Inf</span>, <span class="co"># Read until the end-of-file</span>
                     batch_size <span class="op">=</span> <span class="fl">100</span> <span class="co"># Read text in batches of 100 lines</span>
                     <span class="op">)</span>
<span class="va">freqs</span>
<span class="co">#&gt; A k-gram frequency table.</span></code></pre></div>
<p>The object <code>freqs</code> is an object of class <code>kgram_freqs</code>, i.e. a <span class="math inline">\(k\)</span>-gram frequency table. We can obtain a first informative summary of what this object contains by calling <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">freqs</span><span class="op">)</span>
<span class="co">#&gt; A k-gram frequency table.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt; * N: 5</span>
<span class="co">#&gt; * V: 3046</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of words in training corpus:</span>
<span class="co">#&gt; * W: 26174</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of distinct k-grams with positive counts:</span>
<span class="co">#&gt; * 1-grams:3048</span>
<span class="co">#&gt; * 2-grams:14389</span>
<span class="co">#&gt; * 3-grams:21208</span>
<span class="co">#&gt; * 4-grams:22706</span>
<span class="co">#&gt; * 5-grams:22975</span></code></pre></div>
<p>The parameter <code>V</code> is the size of the dictionary, which was created behind the scenes by <code><a href="../reference/kgram_freqs.html">kgram_freqs()</a></code>, using all words encountered in the text. In alternative, we could have used a pre-specified dictionary through the argument <code>dict</code>, and specify whether new words (not present in the original dictionary) should be added to it, or be replaced by an Unknown-Word (UNK) token, by the argument <code>open_dict</code>; see <code><a href="../reference/kgram_freqs.html">?kgram_freqs</a></code> for further details. The number of distinct unigrams is greater than the size of the dictionary, because the former also includes the special BOS and EOS tokens.</p>
<p>Notice that the functions <code>.preprocess()</code> and <code>.tknz_sent()</code> we defined above are passed as arguments of <code><a href="../reference/kgram_freqs.html">kgram_freqs()</a></code><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. These are also saved in the final <code>kgram_freqs</code> object, and are by default applied also to inputs at prediction time.</p>
<p>The following shows how to query <span class="math inline">\(k\)</span>-gram counts from the frequency table created above:<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Query some simple unigrams and bigrams</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"leonato"</span>, <span class="st">"enter leonato"</span>, <span class="st">"thy"</span>, <span class="st">"smartphones"</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 38  6 52  0</span>
<span class="co"># Query k-grams at the beginning or end of a sentence</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="../reference/EOS.html">BOS</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%+%</span> <span class="fu"><a href="../reference/EOS.html">BOS</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%+%</span> <span class="st">"i"</span>, <span class="st">"love"</span> <span class="op">%+%</span> <span class="fu"><a href="../reference/EOS.html">EOS</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 208   0</span>
<span class="co"># Total number of words processed</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="st">""</span><span class="op">)</span> 
<span class="co">#&gt; [1] 26174</span>
<span class="co"># Total number of sentences processed</span>
<span class="fu"><a href="../reference/query.html">query</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="fu"><a href="../reference/EOS.html">EOS</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> 
<span class="co">#&gt; [1] 2270</span></code></pre></div>
<p>The most important use of <code>kgram_freqs</code> objects is to create language models, as we illustrate in the next step.</p>
</div>
<div id="step-4--build-the-final-language-model" class="section level3">
<h3 class="hasAnchor">
<a href="#step-4--build-the-final-language-model" class="anchor"></a>Step 4. Build the final language model</h3>
<p><code>kgrams</code> provides support for creating language models using several classical smoothing techniques. The list of smoothers currently supported by <code>kgrams</code> can be retrieved through:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/smoothers.html">smoothers</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] "ml"    "add_k" "abs"   "kn"    "mkn"   "sbo"   "wb"</span></code></pre></div>
<p>The documentation page <code><a href="../reference/smoothers.html">?smoothers</a></code> provides a list of original references for the various smoothers. We will use <a href="https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing">Interpolated Kneser-Ney</a> smoothing <span class="citation">(Kneser and Ney 1995; see also Chen and Goodman 1999)</span>, which goes under the code <code>"kn"</code>. We can get some usage help for this method through the command:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/smoothers.html">info</a></span><span class="op">(</span><span class="st">"kn"</span><span class="op">)</span>
<span class="co">#&gt; Interpolated Kneser-Ney</span>
<span class="co">#&gt;  * code: 'kn'</span>
<span class="co">#&gt;  * parameters: D</span>
<span class="co">#&gt;  * constraints: 0 &lt;= D &lt;= 1</span></code></pre></div>
<p>As shown above, Kneser-Ney has one parameter <span class="math inline">\(D\)</span>, which is the discount applied to bare <span class="math inline">\(k\)</span>-gram frequency counts or continuation counts. We will initialize the model with <span class="math inline">\(D = 0.75\)</span>, and later tune this parameter through a test corpus.</p>
<p>To train a language model with the <span class="math inline">\(k\)</span>-gram counts stored in <code>freqs</code>, use:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/language_model.html">language_model</a></span><span class="op">(</span><span class="va">freqs</span>, <span class="st">"kn"</span>, D <span class="op">=</span> <span class="fl">0.75</span><span class="op">)</span>
<span class="va">kn</span>
<span class="co">#&gt; A k-gram language model.</span></code></pre></div>
<p>This will create a <code>language_model</code> object, which can be used to obtain word continuation and sentence probabilities. Let us first get a summary of our final model:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; A k-gram language model.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Smoother:</span>
<span class="co">#&gt; * 'kn'.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt; * N: 5</span>
<span class="co">#&gt; * V: 3046</span>
<span class="co">#&gt; * D: 0.75</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of words in training corpus:</span>
<span class="co">#&gt; * W: 26174</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of distinct k-grams with positive counts:</span>
<span class="co">#&gt; * 1-grams:3048</span>
<span class="co">#&gt; * 2-grams:14389</span>
<span class="co">#&gt; * 3-grams:21208</span>
<span class="co">#&gt; * 4-grams:22706</span>
<span class="co">#&gt; * 5-grams:22975</span></code></pre></div>
<p>The parameter <code>D</code> can be accessed and modified through the functions <code><a href="../reference/parameters.html">parameters()</a></code> and <code><a href="../reference/parameters.html">param()</a></code>, which have a similar interface to the base R function <code><a href="https://rdrr.io/r/base/attributes.html">attributes()</a></code> and <code><a href="https://rdrr.io/r/base/attr.html">attr()</a></code>:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/parameters.html">parameters</a></span><span class="op">(</span><span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; $N</span>
<span class="co">#&gt; [1] 5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $V</span>
<span class="co">#&gt; [1] 3046</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $D</span>
<span class="co">#&gt; [1] 0.75</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.75</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.6</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">0.75</span></code></pre></div>
<p>We can also modify the order of the <span class="math inline">\(k\)</span>-gram model, by choosing any number less than or equal to <span class="math inline">\(N = 5\)</span> (since we stored up to <span class="math inline">\(5\)</span>-gram counts):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="co"># 'kn' uses only 1:4-grams</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span>
<span class="co">#&gt; [1] 4</span>
<span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="co"># 'kn' uses also 5-grams</span></code></pre></div>
<p>In the next section we show how to use this language model for basic tasks such as predicting word and sentence probabilities, and for more complex tasks such as computing perplexities and generating random text.</p>
</div>
</div>
<div id="using-language_model-objects" class="section level2">
<h2 class="hasAnchor">
<a href="#using-language_model-objects" class="anchor"></a>Using <code>language_model</code> objects</h2>
<p>So far we have created a <code>language_model</code> object using Interpolated Kneser-Ney as smoothing method. In this section we show how to:</p>
<ul>
<li>Obtain word continuation and sentence probabilities.</li>
<li>Generate random text by sampling from the language model probability distribution.</li>
<li>Compute the language model’s perplexity on a test corpus.</li>
</ul>
<div id="probability" class="section level3">
<h3 class="hasAnchor">
<a href="#probability" class="anchor"></a>Word continuation and sentence probabilities</h3>
<p>We can obtain both sentence probabilities and word continuation probabilities through the function <code><a href="../reference/probability.html">probability()</a></code>. This is generic on the first argument, which can be a <code>character</code> for sentence probabilities, or a <code>word_context</code> expression for continuation probabilities.</p>
<p>Sentence probabilities can be obtained as follows (the first two are sentences from the training corpus):</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/probability.html">probability</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Did he break out into tears?"</span>,
              <span class="st">"I see, lady, the gentleman is not in your books."</span>,
              <span class="st">"We are predicting sentence probabilities."</span>
              <span class="op">)</span>,
            model <span class="op">=</span> <span class="va">kn</span>
            <span class="op">)</span>
<span class="co">#&gt; [1] 2.720954e-05 8.628460e-07 9.230391e-19</span></code></pre></div>
<p>Behind the scenes, the same <code>.preprocess()</code> and <code>.tknz_sent()</code> functions used during training are being applied to the input. We can turn off this behaviour by explicitly specifying the <code>.preprocess</code> and <code>.tknz_sent</code> arguments of <code><a href="../reference/probability.html">probability()</a></code>.</p>
<p>Word continuation probabilities are the conditional probabilities of words following some given context. For instance, the probability that the words <code>"tears"</code> or <code>"pieces"</code> will follow the context <code>"Did he break out into"</code> are computed as follows:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/probability.html">probability</a></span><span class="op">(</span><span class="st">"tears"</span> <span class="op">%|%</span> <span class="st">"Did he break out into"</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.5813743</span>
<span class="fu"><a href="../reference/probability.html">probability</a></span><span class="op">(</span><span class="st">"pieces"</span> <span class="op">%|%</span> <span class="st">"Did he break out into"</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; [1] 9.992621e-06</span></code></pre></div>
<p>The operator <code><a href="../reference/word_context.html">%|%</a></code> takes as input a character vector on its left-hand side, i.e. the list of candidate words, and a length one character vector on its right-hand side, i.e. the given context. If the context has more than <span class="math inline">\(N - 1\)</span> words (where <span class="math inline">\(N\)</span> is the order of the language model, five in our case), only the last <span class="math inline">\(N - 1\)</span> words are used for prediction.</p>
</div>
<div id="sampling" class="section level3">
<h3 class="hasAnchor">
<a href="#sampling" class="anchor"></a>Generating random text</h3>
<p>We can sample sentences from the probability distribution defined by our language model using <code><a href="../reference/sample_sentences.html">sample_sentences()</a></code>. For instance:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">840</span><span class="op">)</span>
<span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "i have studied officers ; &lt;EOS&gt;"                                              </span>
<span class="co">#&gt;  [2] "truly by in your company thing that you ask for [...] (truncated output)"     </span>
<span class="co">#&gt;  [3] "i protest i love the gentleman is wise ; &lt;EOS&gt;"                               </span>
<span class="co">#&gt;  [4] "for it . &lt;EOS&gt;"                                                               </span>
<span class="co">#&gt;  [5] "the best befits can i for your own hobbyhorses hence [...] (truncated output)"</span>
<span class="co">#&gt;  [6] "but by this travail fit the length july cham's beard [...] (truncated output)"</span>
<span class="co">#&gt;  [7] "don pedro she doth well as being some attires and [...] (truncated output)"   </span>
<span class="co">#&gt;  [8] "exeunt all ladies only spots of grey all the wealth [...] (truncated output)" </span>
<span class="co">#&gt;  [9] "heighho ! &lt;EOS&gt;"                                                              </span>
<span class="co">#&gt; [10] "exit margaret ursula friar . &lt;EOS&gt;"</span></code></pre></div>
<p>The sampling is performed word by word, and the output is truncated if no <code>EOS</code> token is found after sampling <code>max_length</code> words.</p>
<p>We can also sample with a temperature different from one. The temperature transformation of a probability distribution <span class="math inline">\(p(i)\)</span> is defined by:</p>
<p><span class="math display">\[p_t(i) = \dfrac{\exp(\log{p(i)} / t)} {Z(t)},\]</span> where <span class="math inline">\(Z(t)\)</span> is the partition function, defined in such a way that <span class="math inline">\(\sum _i p_t(i) \equiv 1\)</span>. Intuitively, higher and lower temperatures make the original probability distribution smoother and rougher, respectively. By making a physical analogy, we can think of less probable words as states with higher energies, and the effect of higher (lower) temperatures is to make more (less) likely to excite these high energy states.</p>
<p>We can test the effects of temperature on our Shakespeare-inspired language model, by changing the parameter <code>t</code> of <code><a href="../reference/sample_sentences.html">sample_sentences()</a></code> (notice that the default <code>t = 1</code> corresponds to the original distribution):</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>, 
                 t <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># low temperature</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "i will not have to do with you . &lt;EOS&gt;"                                    </span>
<span class="co">#&gt;  [2] "i will go before and show him their examination . [...] (truncated output)"</span>
<span class="co">#&gt;  [3] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [4] "i will not have to do with you . &lt;EOS&gt;"                                    </span>
<span class="co">#&gt;  [5] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [6] "i will not think it . &lt;EOS&gt;"                                               </span>
<span class="co">#&gt;  [7] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [8] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt;  [9] "i will not be sworn but love may transform me [...] (truncated output)"    </span>
<span class="co">#&gt; [10] "i will not have to do with you . &lt;EOS&gt;"</span>
<span class="fu"><a href="../reference/sample_sentences.html">sample_sentences</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">kn</span>, 
                 n <span class="op">=</span> <span class="fl">10</span>,
                 max_length <span class="op">=</span> <span class="fl">10</span>, 
                 t <span class="op">=</span> <span class="fl">10</span> <span class="co"># high temperature</span>
                 <span class="op">)</span>
<span class="co">#&gt;  [1] "pleasantspirited minds wants valuing peace speech libertines after offered being [...] (truncated output)"     </span>
<span class="co">#&gt;  [2] "braggarts smell fiveandthirty from possible knowest sickness tonight panders agony [...] (truncated output)"   </span>
<span class="co">#&gt;  [3] "show'd where's give intelligence princes finer tire scab brought rearward [...] (truncated output)"            </span>
<span class="co">#&gt;  [4] "deserved heart's evening virtues holds c hadst persuasion can finer [...] (truncated output)"                  </span>
<span class="co">#&gt;  [5] "churchbench modesty thinks noncome remorse epitaphs consented mortifying whom hath [...] (truncated output)"   </span>
<span class="co">#&gt;  [6] "expectation impossible yielded deceive wedding mouth unclasp absentand qualify twelve [...] (truncated output)"</span>
<span class="co">#&gt;  [7] "giddily certainly nightraven prized grief laugh claw invincible tyrant blessed [...] (truncated output)"       </span>
<span class="co">#&gt;  [8] "'i senseless beat time denies 'hundred ten forth hire' reenter [...] (truncated output)"                       </span>
<span class="co">#&gt;  [9] "toothache laughed civil kill'd mean hero's yea foundation deformed appetite [...] (truncated output)"          </span>
<span class="co">#&gt; [10] "hour studied figure nine leonato enough ever herself confess authority [...] (truncated output)"</span></code></pre></div>
<p>As explained above, sampling with low temperature gives much more weight to probable sentences, and indeed the output is very repetitive. On the contrary, high temperature makes sentence probabilities more uniform, and in fact our output above looks quite random.</p>
</div>
<div id="perplexity" class="section level3">
<h3 class="hasAnchor">
<a href="#perplexity" class="anchor"></a>Compute language model’s perplexities</h3>
<p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is a standard evaluation metric for the overall performance of a language model. It is given by <span class="math inline">\(P = e^H\)</span>, where <span class="math inline">\(H\)</span> is the cross-entropy of the language model sentence probability distribution against a test corpus empirical distribution:</p>
<p><span class="math display">\[
H = - \dfrac{1}{W}\sum _s\ \ln (\text {Prob}(s)) 
\]</span> Here <span class="math inline">\(W\)</span> is total number of words in the test corpus (following Ref. <span class="citation">(Chen and Goodman 1999)</span>, we include counts of the EOS token, but not the BOS token, in <span class="math inline">\(W\)</span>), and the sum extends over all sentences in the test corpus. Perplexity does not give direct information on the performance of a language model in a specific end-to-end task, but is often found to correlate with the latter, which provides a practical justification for the use of this metric. Notice that better models are associated with lower perplexities, and that <span class="math inline">\(H\)</span> is proportional to the negative log-likelihood of the corpus under the language model assumption.</p>
<p>Perplexities can be computed in <code>kgrams</code> using the function <code><a href="../reference/perplexity.html">perplexity()</a></code>, which can read text both from a <code>character</code> vector and from a <code>connection</code>. We will take our test corpus again from Shakespeare’s opus, specifically the play “A Midsummer Night’s Dream”, which is example data from <code>kgrams</code> namespace:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">midsummer</span><span class="op">[</span><span class="fl">840</span><span class="op">]</span>
<span class="co">#&gt; [1] " when truth kills truth o devilishholy fray !"</span></code></pre></div>
<p>We can compute the perplexity of our Kneser-Ney <span class="math inline">\(5\)</span>-gram model <code>kn</code> against this corpus as follows:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/perplexity.html">perplexity</a></span><span class="op">(</span><span class="va">midsummer</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="co">#&gt; [1] 376.8554</span></code></pre></div>
<p>We can use perplexity to tune our model parameter <span class="math inline">\(D\)</span>. We compute perplexity over a grid of values for <code>D</code> and plot the results. We do this for the <span class="math inline">\(k\)</span>-gram models with <span class="math inline">\(k \in \{2, 3, 4, 5\}\)</span>:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">D_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.5</span>, to <span class="op">=</span> <span class="fl">1.0</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span>
<span class="va">FUN</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">D</span>, <span class="va">N</span><span class="op">)</span> <span class="op">{</span>
        <span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"N"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">N</span>
        <span class="fu"><a href="../reference/parameters.html">param</a></span><span class="op">(</span><span class="va">kn</span>, <span class="st">"D"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">D</span>
        <span class="fu"><a href="../reference/perplexity.html">perplexity</a></span><span class="op">(</span><span class="va">midsummer</span>, model <span class="op">=</span> <span class="va">kn</span><span class="op">)</span>
<span class="op">}</span>
<span class="va">P_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">2</span><span class="op">:</span><span class="fl">5</span>, <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">FUN</span>, N <span class="op">=</span> <span class="va">N</span><span class="op">)</span><span class="op">)</span>
<span class="va">oldpar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"n"</span>, xlab <span class="op">=</span> <span class="st">"D"</span>, ylab <span class="op">=</span> <span class="st">"Perplexity"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">300</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"chartreuse"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">3</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">D_grid</span>, <span class="va">P_grid</span><span class="op">[[</span><span class="fl">4</span><span class="op">]</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="kgrams_files/figure-html/unnamed-chunk-19-1.png" alt="Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models." width="50%"><p class="caption">
Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models.
</p>
</div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span><span class="va">oldpar</span><span class="op">)</span></code></pre></div>
<p>We see that the optimal choices for <code>D</code> are close to its maximum allowed value <code>D = 1</code>, for which the performance of the 2-gram model is slightly worse than the higher order models, and that the 5-gram model performs generally worse than the 3-gram and 4-gram models. Indeed, the optimized perplexities for the various <span class="math inline">\(k\)</span>-gram orders are given by:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"2-gram"</span> <span class="op">=</span> <span class="fl">1</span>, <span class="st">"3-gram"</span> <span class="op">=</span> <span class="fl">2</span>, <span class="st">"4-gram"</span> <span class="op">=</span> <span class="fl">3</span>, <span class="st">"5-gram"</span> <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>,
       <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">P_grid</span><span class="op">[[</span><span class="va">N</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>
       <span class="op">)</span>
<span class="co">#&gt;   2-gram   3-gram   4-gram   5-gram </span>
<span class="co">#&gt; 324.8595 320.5757 319.9021 319.9982</span></code></pre></div>
<p>which shows that the best performing model is the 4-gram one, while it seems that the 5-gram model is starting to overfit (which is not very surprising, given the ridiculously small size of our training corpus!).</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2 class="hasAnchor">
<a href="#conclusions" class="anchor"></a>Conclusions</h2>
<p>In this vignette I have shown how to implement and explore <span class="math inline">\(k\)</span>-gram language models in R using <code>kgrams</code>. For further help, you can consult the reference page of the <code>kgrams</code> <a href="https://vgherard.github.io/kgrams/">website</a>. Development of <code>kgrams</code> takes place on its <a href="https://github.com/vgherard/kgrams/">GitHub repository</a>. If you find a bug, please let me know by opening an issue on GitHub, and if you have any ideas or proposals for improvement, please feel welcome to send a pull request, or simply an e-mail at <a href="mailto:vgherard@sissa.it" class="email">vgherard@sissa.it</a>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-chen1999empirical">
<p>Chen, Stanley F, and Joshua Goodman. 1999. “An Empirical Study of Smoothing Techniques for Language Modeling.” <em>Computer Speech &amp; Language</em> 13 (4): 359–94.</p>
</div>
<div id="ref-Kneser1995ImprovedBF">
<p>Kneser, Reinhard, and H. Ney. 1995. “Improved Backing-Off for M-Gram Language Modeling.” <em>1995 International Conference on Acoustics, Speech, and Signal Processing</em> 1: 181–84 vol.1.</p>
</div>
<div id="ref-Pibiri_2019">
<p>Pibiri, Giulio Ermanno, and Rossano Venturini. 2019. “Handling Massive N -Gram Datasets Efficiently.” <em>ACM Transactions on Information Systems</em> 37 (2): 1–41. <a href="https://doi.org/10.1145/3302913">https://doi.org/10.1145/3302913</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Here and below, when we talk about “language models”, we always refer to <em>word-level</em> language models. In particular, a <span class="math inline">\(k\)</span>-gram is a <span class="math inline">\(k\)</span>-tuple of words.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is a standard evaluation metric for language models, based on the model’s sentence probability distribution cross-entropy with the empirical distribution of a test corpus. It is described in some more detail in this <a href="#perplexity">Subsection</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Strictly speaking, a single argument <code>.preprocess</code> would suffice, as the processed input is (symbolically) <code>.tknz_sent(.preprocess(input))</code>. Having two separate arguments for preprocessing and sentence tokenization has a couple of advantages, as explained in <code><a href="../reference/kgram_freqs.html">?kgram_freqs</a></code>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The string concatenation operator <code><a href="../reference/%+%.html">%+%</a></code> is equivalent to <code><a href="https://rdrr.io/r/base/paste.html">paste(lhs, rhs)</a></code>. Also, the helpers <code><a href="../reference/EOS.html">BOS()</a></code>, <code><a href="../reference/EOS.html">EOS()</a></code> and <code><a href="../reference/EOS.html">UNK()</a></code> return the BOS, EOS and UNK tokens, respectively.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Valerio Gherardi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
